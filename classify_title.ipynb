{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adfba29-1fb8-40d7-a35b-01322509e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/envs/lexscraper/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "from scraping import (\n",
    "    parse_podcats\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd321c84-9f8b-4043-a786-c136785974ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_podcasts = \"https://lexfridman.com/podcast\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ac44c",
   "metadata": {},
   "source": [
    "Retrieving of the podcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbac75cf-d5a7-43a1-92b5-091ab18de698",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url_podcasts)\n",
    "html_content = res.content\n",
    "soup_podcasts = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "podcasts = parse_podcats(soup_podcasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32169571-8ef4-420a-b6a1-673440f5bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [p.title for p in podcasts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a832ed6",
   "metadata": {},
   "source": [
    "First attempt at classifying the podcasts based on their title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed87427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\" \n",
    "# model_name = \"distilbert-base-uncased\" \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "labels = ['related to AI', 'not related to AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ade93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics, Family, Real Estate, Fashion, Music, and Life\n",
      "not related to AI\n",
      "Focus, Controversy, Politics, and Relationships\n",
      "not related to AI\n",
      "Perplexity CEO on Future of AI, Search & the Internet\n",
      "not related to AI\n",
      "Physics of Life, Time, Complexity, and Aliens\n",
      "not related to AI\n",
      "Power, Controversy, Betrayal, Truth & Love in Film and Life\n",
      "not related to AI\n",
      "Dangers of Superintelligent AI\n",
      "not related to AI\n",
      "Human Memory, Imagination, Deja Vu, and False Memories\n",
      "not related to AI\n",
      "Jungle, Apex Predators, Aliens, Uncontacted Tribes, and God\n",
      "not related to AI\n",
      "General Relativity, Quantum Mechanics, Black Holes & Aliens\n",
      "not related to AI\n",
      "Judo, Olympics, Winning, Losing, and the Champion Mindset\n",
      "not related to AI\n"
     ]
    }
   ],
   "source": [
    "for title in titles[:10]:\n",
    "\n",
    "    inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs) \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    label = labels[predicted_class]\n",
    "\n",
    "    print(f'{title}\\n{label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c3fe0",
   "metadata": {},
   "source": [
    "**Plan of action:**\n",
    "\n",
    "- Fine tune distilled Bert.\n",
    "    - create a training set.\n",
    "        - Use guest to labellize past titles.\n",
    "        - Use traduction and synonms to do data augmentation.\n",
    "        - scrap some data.\n",
    "    - Do the fine tuning\n",
    "- Test it of a validation set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf032e27",
   "metadata": {},
   "source": [
    "#### Training dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a038dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_podcast = pd.DataFrame(podcasts)\n",
    "df_podcast.insert(0, 'id', df_podcast.reset_index(drop=True).index + 1)\n",
    "\n",
    "cols_to_pp = [\"guest\", \"title\"]\n",
    "\n",
    "for col in cols_to_pp:\n",
    "    df_podcast[col] = np.vectorize(lambda title: unidecode(title))(df_podcast[col])\n",
    "    df_podcast[col] = df_podcast[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de5a9b",
   "metadata": {},
   "source": [
    "The positive titles will be:\n",
    "- titles of podcast whose guest is a AI/ML/DS person I am interested in.\n",
    "\n",
    "or\n",
    "\n",
    "- titles with ML terms that I will validate by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de92e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_guests = [\n",
    "    \"Aravind Srinivas\",\n",
    "    \"Sam Altman\",\n",
    "    \"Yann LeCun\",\n",
    "    \"Joscha Bach\",\n",
    "    \"Max Tegmark\",\n",
    "    \"Noam Brown\",\n",
    "    \"Rana el Kaliouby\",\n",
    "    \"Ray Kurzweil\",\n",
    "    \"Oriol Vinyals\",\n",
    "    \"Demis Hassabis\",\n",
    "    \"Travis Oliphant\",\n",
    "    \"Jay McClelland\",\n",
    "    \"Douglas Lenat\",\n",
    "    \"Wojciech Zaremba\",\n",
    "    \"Ishan Misra\",\n",
    "    \"Risto Miikkulainen\",\n",
    "    \"Max Tegmark\",\n",
    "    \"Dan Kokotov\",\n",
    "    \"Michael Littman\",\n",
    "    \"Charles Isbell\",\n",
    "    \"Fran√ßois Chollet\",\n",
    "    \"Dileep George\",\n",
    "    \"Jitendra Malik\",\n",
    "    \"Sergey Levine\",\n",
    "    \"Matt Botvinick\",\n",
    "    \"Ben Goertzel\",\n",
    "    \"Dawn Song\",\n",
    "    \"Ilya Sutskever\",\n",
    "    \"Daphne Koller\",\n",
    "    \"David Silver\",\n",
    "    \"Marcus Hutter\",\n",
    "    \"Michael I. Jordan\",\n",
    "    \"Andrew Ng\",\n",
    "    \"Gary Marcus\",\n",
    "    \"Peter Norvig\",\n",
    "    \"Regina Barzilay\",\n",
    "    \"Jeremy Howard\",\n",
    "    \"Rajat Monga\",\n",
    "    \"Ian Goodfellow\",\n",
    "    \"Greg Brockman\",\n",
    "    \"Tomaso Poggio\",\n",
    "    \"Juergen Schmidhuber\",\n",
    "    \"Pieter Abbeel\",\n",
    "    \"Stuart Russell\",\n",
    "    \"Yoshua Bengio\",\n",
    "    \"Vladimir Vapnik\",\n",
    "]\n",
    "data_science_guests_pp = [unidecode(guest).lower() for guest in data_science_guests]\n",
    "\n",
    "words_to_check = [\n",
    "    \"Neural Nets\",\n",
    "    \"neural networks\",\n",
    "    \"Deep Learning\",\n",
    "    \"Machine learning\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Data science\",\n",
    "    \"AI\",\n",
    "    \"AGI\",\n",
    "    \"artificial intelligence\"\n",
    "]\n",
    "words_to_check_pp = [unidecode(word).lower() for word in words_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeec3bf",
   "metadata": {},
   "source": [
    "Manual validation based on regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20a21555",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_not_character_or_beginning = r'(?:[^a-z]|^)'\n",
    "re_not_character_or_end = r'(?:[^a-z]|$)'\n",
    "\n",
    "pattern = (\n",
    "    re_not_character_or_beginning +\n",
    "    (f'{re_not_character_or_end}|{re_not_character_or_beginning}').join(words_to_check_pp) + \n",
    "    re_not_character_or_end\n",
    ")\n",
    "df_podcast[\"has_ml_word\"] = df_podcast.title.str.contains(pattern, case=False, regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5d726283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: roman yampolskiy\n",
      "dangers of superintelligent ai\n",
      "\n",
      "30: guillaume verdon\n",
      "beff jezos, e/acc movement, physics, computation & agi\n",
      "\n",
      "37: elon musk\n",
      "war, ai, aliens, politics, physics, video games, and humanity\n",
      "\n",
      "50: george hotz\n",
      "tiny corp, twitter, ai safety, self-driving, and god\n",
      "\n",
      "51: marc andreessen\n",
      "future of the internet, technology, and ai\n",
      "\n",
      "54: mark zuckerberg\n",
      "future of ai at meta, facebook, instagram, and whatsapp\n",
      "\n",
      "56: chris lattner\n",
      "future of programming and ai\n",
      "\n",
      "64: manolis kellis\n",
      "evolution of human civilization and superintelligent ai\n",
      "\n",
      "69: eliezer yudkowsky\n",
      "dangers of ai and the end of human civilization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "increment = 9\n",
    "\n",
    "df = df_podcast.loc[\n",
    "    (~df_podcast.guest.isin(data_science_guests_pp)) & (df_podcast.has_ml_word == 1)\n",
    "]\n",
    "\n",
    "for _, (id, guest, title) in df.loc[:,[\"id\", \"guest\", \"title\"]].iloc[batch*increment: batch*increment + increment].iterrows():\n",
    "    print(f\"{id}: {guest}\\n{title}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_positive_id = [\n",
    "\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
