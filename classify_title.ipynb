{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adfba29-1fb8-40d7-a35b-01322509e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/envs/lexscraper/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import requests\n",
    "from itertools import product\n",
    "from html import unescape\n",
    "\n",
    "from unidecode import unidecode\n",
    "from typing import Sequence\n",
    "\n",
    "import translators as ts\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from scraping import (\n",
    "    parse_podcats\n",
    ")\n",
    "from nlp_utils import (\n",
    "    hash_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd321c84-9f8b-4043-a786-c136785974ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_podcasts = \"https://lexfridman.com/podcast\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ac44c",
   "metadata": {},
   "source": [
    "Retrieving of the podcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbac75cf-d5a7-43a1-92b5-091ab18de698",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url_podcasts)\n",
    "html_content = res.content\n",
    "soup_podcasts = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "podcasts = parse_podcats(soup_podcasts)\n",
    "titles = [p.title for p in podcasts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a832ed6",
   "metadata": {},
   "source": [
    "First attempt at classifying the podcasts based on their title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed87427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\" \n",
    "# model_name = \"distilbert-base-uncased\" \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "labels = ['related to AI', 'not related to AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ade93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics, Family, Real Estate, Fashion, Music, and Life\n",
      "not related to AI\n",
      "Focus, Controversy, Politics, and Relationships\n",
      "not related to AI\n",
      "Perplexity CEO on Future of AI, Search & the Internet\n",
      "not related to AI\n",
      "Physics of Life, Time, Complexity, and Aliens\n",
      "not related to AI\n",
      "Power, Controversy, Betrayal, Truth & Love in Film and Life\n",
      "not related to AI\n",
      "Dangers of Superintelligent AI\n",
      "not related to AI\n",
      "Human Memory, Imagination, Deja Vu, and False Memories\n",
      "not related to AI\n",
      "Jungle, Apex Predators, Aliens, Uncontacted Tribes, and God\n",
      "not related to AI\n",
      "General Relativity, Quantum Mechanics, Black Holes & Aliens\n",
      "not related to AI\n",
      "Judo, Olympics, Winning, Losing, and the Champion Mindset\n",
      "not related to AI\n"
     ]
    }
   ],
   "source": [
    "for title in titles[:10]:\n",
    "\n",
    "    inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs) \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    label = labels[predicted_class]\n",
    "\n",
    "    print(f'{title}\\n{label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c3fe0",
   "metadata": {},
   "source": [
    "**Plan of action:**\n",
    "\n",
    "- Fine tune distilled Bert.\n",
    "    - create a training set.\n",
    "        - Use guest to labellize past titles.\n",
    "        - Use traduction and synonms to do data augmentation.\n",
    "        - scrap some data.\n",
    "    - Do the fine tuning\n",
    "- Test it of a validation set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf032e27",
   "metadata": {},
   "source": [
    "#### Training dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a038dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_podcast = pd.DataFrame(podcasts)\n",
    "\n",
    "\n",
    "cols_to_pp = [\"guest\", \"title\"]\n",
    "\n",
    "for col in cols_to_pp:\n",
    "    df_podcast[col] = np.vectorize(lambda title: unidecode(title))(df_podcast[col])\n",
    "    df_podcast[col] = df_podcast[col].str.lower()\n",
    "\n",
    "# df_podcast.insert(0, 'id', df_podcast.reset_index(drop=True).index + 1)\n",
    "hash_str_vec = np.vectorize(hash_str)\n",
    "df_podcast.insert(0, 'id', hash_str_vec(df_podcast.guest + df_podcast.title))\n",
    "\n",
    "assert df_podcast.id.nunique() == len(df_podcast), \"colision in the hashing to create unique id!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de5a9b",
   "metadata": {},
   "source": [
    "The positive titles will be:\n",
    "- titles of podcast whose guest is a AI/ML/DS person I am interested in.\n",
    "\n",
    "or\n",
    "\n",
    "- titles with ML terms that I will validate by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de92e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_guests = [\n",
    "    \"Aravind Srinivas\",\n",
    "    \"Andrej Karpathy\",\n",
    "    \"Sam Altman\",\n",
    "    \"Yann LeCun\",\n",
    "    \"Joscha Bach\",\n",
    "    \"Max Tegmark\",\n",
    "    \"Noam Brown\",\n",
    "    \"Rana el Kaliouby\",\n",
    "    \"Ray Kurzweil\",\n",
    "    \"Oriol Vinyals\",\n",
    "    \"Demis Hassabis\",\n",
    "    \"Travis Oliphant\",\n",
    "    \"Jay McClelland\",\n",
    "    \"Douglas Lenat\",\n",
    "    \"Wojciech Zaremba\",\n",
    "    \"Ishan Misra\",\n",
    "    \"Risto Miikkulainen\",\n",
    "    \"Max Tegmark\",\n",
    "    \"Dan Kokotov\",\n",
    "    \"Michael Littman\",\n",
    "    \"Charles Isbell\",\n",
    "    \"FranÃ§ois Chollet\",\n",
    "    \"Dileep George\",\n",
    "    \"Jitendra Malik\",\n",
    "    \"Sergey Levine\",\n",
    "    \"Matt Botvinick\",\n",
    "    \"Ben Goertzel\",\n",
    "    \"Dawn Song\",\n",
    "    \"Ilya Sutskever\",\n",
    "    \"Daphne Koller\",\n",
    "    \"David Silver\",\n",
    "    \"Marcus Hutter\",\n",
    "    \"Michael I. Jordan\",\n",
    "    \"Andrew Ng\",\n",
    "    \"Gary Marcus\",\n",
    "    \"Peter Norvig\",\n",
    "    \"Regina Barzilay\",\n",
    "    \"Jeremy Howard\",\n",
    "    \"Rajat Monga\",\n",
    "    \"Ian Goodfellow\",\n",
    "    \"Greg Brockman\",\n",
    "    \"Tomaso Poggio\",\n",
    "    \"Juergen Schmidhuber\",\n",
    "    \"Pieter Abbeel\",\n",
    "    \"Stuart Russell\",\n",
    "    \"Yoshua Bengio\",\n",
    "    \"Vladimir Vapnik\",\n",
    "]\n",
    "data_science_guests_pp = [unidecode(guest).lower() for guest in data_science_guests]\n",
    "\n",
    "words_to_check = [\n",
    "    \"Neural Nets\",\n",
    "    \"neural networks\",\n",
    "    \"Deep Learning\",\n",
    "    \"Machine learning\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Data science\",\n",
    "    \"AI\",\n",
    "    \"AGI\",\n",
    "    \"artificial intelligence\"\n",
    "]\n",
    "words_to_check_pp = [unidecode(word).lower() for word in words_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeec3bf",
   "metadata": {},
   "source": [
    "Manual validation based on regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a21555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_guest_relevant(guest_candidate: str, data_science_guests: Sequence[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if any data science guest is contained in the given guest_candidate.\n",
    "    Necessary because some guest_candidate contained multiple guests.\n",
    "    \"\"\"\n",
    "    return any(relevant_guest in guest_candidate for relevant_guest in data_science_guests)\n",
    "\n",
    "is_guest_relevant_vec = np.vectorize(\n",
    "    is_guest_relevant,\n",
    "    excluded=['data_science_guests'],\n",
    "    signature='(),(n)->()'\n",
    ")\n",
    "df_podcast[\"has_relevant_guest\"] = is_guest_relevant_vec(df_podcast.guest, data_science_guests_pp).astype(int)\n",
    "\n",
    "re_not_character_or_beginning = r'(?:[^a-z]|^)'\n",
    "re_not_character_or_end = r'(?:[^a-z]|$)'\n",
    "\n",
    "pattern = (\n",
    "    re_not_character_or_beginning +\n",
    "    (f'{re_not_character_or_end}|{re_not_character_or_beginning}').join(words_to_check_pp) + \n",
    "    re_not_character_or_end\n",
    ")\n",
    "df_podcast[\"has_ml_word\"] = df_podcast.title.str.contains(pattern, case=False, regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5d726283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98195452: roman yampolskiy\n",
      "dangers of superintelligent ai\n",
      "\n",
      "86440844: guillaume verdon\n",
      "beff jezos, e/acc movement, physics, computation & agi\n",
      "\n",
      "68071720: elon musk\n",
      "war, ai, aliens, politics, physics, video games, and humanity\n",
      "\n",
      "37124594: george hotz\n",
      "tiny corp, twitter, ai safety, self-driving, and god\n",
      "\n",
      "60879511: marc andreessen\n",
      "future of the internet, technology, and ai\n",
      "\n",
      "99072863: mark zuckerberg\n",
      "future of ai at meta, facebook, instagram, and whatsapp\n",
      "\n",
      "41763735: chris lattner\n",
      "future of programming and ai\n",
      "\n",
      "68276965: manolis kellis\n",
      "evolution of human civilization and superintelligent ai\n",
      "\n",
      "62663336: eliezer yudkowsky\n",
      "dangers of ai and the end of human civilization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "increment = 9\n",
    "\n",
    "df_to_check = df_podcast.loc[\n",
    "    (df_podcast.has_relevant_guest == 0) & (df_podcast.has_ml_word == 1) \n",
    "]\n",
    "\n",
    "for _, (id, guest, title) in df_to_check.loc[:,[\"id\", \"guest\", \"title\"]].iloc[batch*increment: batch*increment + increment].iterrows():\n",
    "    print(f\"{id}: {guest}\\n{title}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5415d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_positive_id = [\n",
    "    98195452,\n",
    "    41763735,\n",
    "    68276965,\n",
    "    59789751,\n",
    "    60449962,\n",
    "    93271390,\n",
    "    79695739,\n",
    "    31945141,\n",
    "    58779362,\n",
    "]\n",
    "\n",
    "df_podcast[\"manual_label\"] = df_podcast.id.isin(manual_positive_id).astype(int)\n",
    "\n",
    "df_podcast[\"label\"] = (\n",
    "    df_podcast.manual_label | df_podcast.has_relevant_guest\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "58bd74bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guest</th>\n",
       "      <th>title</th>\n",
       "      <th>url_transcript</th>\n",
       "      <th>has_ml_word</th>\n",
       "      <th>has_relevant_guest</th>\n",
       "      <th>manual_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>86694947</td>\n",
       "      <td>richard wrangham</td>\n",
       "      <td>violence, sex, and fire in human evolution</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>11647340</td>\n",
       "      <td>francois chollet</td>\n",
       "      <td>keras, deep learning, and the progress of ai</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>46475578</td>\n",
       "      <td>chris duffin</td>\n",
       "      <td>powerlifting and the engineering of strength</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>48181633</td>\n",
       "      <td>dan gable</td>\n",
       "      <td>olympic wrestling</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>86440844</td>\n",
       "      <td>guillaume verdon</td>\n",
       "      <td>beff jezos, e/acc movement, physics, computati...</td>\n",
       "      <td>https://lexfridman.com/guillaume-verdon-transc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>59789751</td>\n",
       "      <td>charles isbell and michael littman</td>\n",
       "      <td>machine learning and education</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>46097646</td>\n",
       "      <td>russ tedrake</td>\n",
       "      <td>underactuated robotics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>39332756</td>\n",
       "      <td>andrew ng</td>\n",
       "      <td>deep learning, education, and real-world ai</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>60879511</td>\n",
       "      <td>marc andreessen</td>\n",
       "      <td>future of the internet, technology, and ai</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>34629302</td>\n",
       "      <td>francis collins</td>\n",
       "      <td>national institutes of health</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                               guest  \\\n",
       "207  86694947                    richard wrangham   \n",
       "397  11647340                    francois chollet   \n",
       "229  46475578                        chris duffin   \n",
       "284  48181633                           dan gable   \n",
       "29   86440844                    guillaume verdon   \n",
       "288  59789751  charles isbell and michael littman   \n",
       "322  46097646                        russ tedrake   \n",
       "362  39332756                           andrew ng   \n",
       "50   60879511                     marc andreessen   \n",
       "198  34629302                     francis collins   \n",
       "\n",
       "                                                 title  \\\n",
       "207         violence, sex, and fire in human evolution   \n",
       "397       keras, deep learning, and the progress of ai   \n",
       "229       powerlifting and the engineering of strength   \n",
       "284                                  olympic wrestling   \n",
       "29   beff jezos, e/acc movement, physics, computati...   \n",
       "288                     machine learning and education   \n",
       "322                             underactuated robotics   \n",
       "362        deep learning, education, and real-world ai   \n",
       "50          future of the internet, technology, and ai   \n",
       "198                      national institutes of health   \n",
       "\n",
       "                                        url_transcript  has_ml_word  \\\n",
       "207                                               None            0   \n",
       "397                                               None            1   \n",
       "229                                               None            0   \n",
       "284                                               None            0   \n",
       "29   https://lexfridman.com/guillaume-verdon-transc...            1   \n",
       "288                                               None            1   \n",
       "322                                               None            0   \n",
       "362                                               None            1   \n",
       "50                                                None            1   \n",
       "198                                               None            0   \n",
       "\n",
       "     has_relevant_guest  manual_label  label  \n",
       "207                   0             0      0  \n",
       "397                   1             0      1  \n",
       "229                   0             0      0  \n",
       "284                   0             0      0  \n",
       "29                    0             0      0  \n",
       "288                   1             1      1  \n",
       "322                   0             0      0  \n",
       "362                   1             0      1  \n",
       "50                    0             0      0  \n",
       "198                   0             0      0  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_podcast.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0a753",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd540e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pierre/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pierre/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee317de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copycat', 'caricature', 'emulator', 'anthropoid', 'imitator', 'aper']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms(\"ape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e0ad8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn: Synset('ape.n.01')\n",
      "lemmas: [Lemma('ape.n.01.ape')]\n",
      "lemma name: ['ape']\n",
      "\n",
      "syn: Synset('copycat.n.01')\n",
      "lemmas: [Lemma('copycat.n.01.copycat'), Lemma('copycat.n.01.imitator'), Lemma('copycat.n.01.emulator'), Lemma('copycat.n.01.ape'), Lemma('copycat.n.01.aper')]\n",
      "lemma name: ['copycat', 'imitator', 'emulator', 'ape', 'aper']\n",
      "\n",
      "syn: Synset('anthropoid.n.01')\n",
      "lemmas: [Lemma('anthropoid.n.01.anthropoid'), Lemma('anthropoid.n.01.ape')]\n",
      "lemma name: ['anthropoid', 'ape']\n",
      "\n",
      "syn: Synset('ape.v.01')\n",
      "lemmas: [Lemma('ape.v.01.ape')]\n",
      "lemma name: ['ape']\n",
      "\n",
      "syn: Synset('caricature.v.01')\n",
      "lemmas: [Lemma('caricature.v.01.caricature'), Lemma('caricature.v.01.ape')]\n",
      "lemma name: ['caricature', 'ape']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"ape\"\n",
    "\n",
    "syns = wn.synsets(word)\n",
    "\n",
    "for syn in syns:\n",
    "\n",
    "    print(f\"syn: {syn}\")\n",
    "    lemmas = syn.lemmas()\n",
    "\n",
    "    print(f\"lemmas: {lemmas}\")\n",
    "    print(f\"lemma name: {[lemma.name() for lemma in lemmas]}\")\n",
    "    print('\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b59c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a3fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'cable_car',\n",
       " 'gondola',\n",
       " 'motorcar',\n",
       " 'railcar',\n",
       " 'railroad_car',\n",
       " 'railway_car',\n",
       " 'elevator_car',\n",
       " 'automobile',\n",
       " 'auto']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    np.random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # only replace up to n words\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a048dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetCorpusReader in '.../corpora/wordnet' (not loaded yet)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe48506",
   "metadata": {},
   "source": [
    "##### Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3ce407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alibaba', 'apertium', 'argos', 'baidu', 'bing', 'caiyun', 'cloudTranslation', 'deepl', 'elia', 'google', 'hujiang', 'iciba', 'iflytek', 'iflyrec', 'itranslate', 'judic', 'languageWire', 'lingvanex', 'niutrans', 'mglip', 'mirai', 'modernMt', 'myMemory', 'papago', 'qqFanyi', 'qqTranSmart', 'reverso', 'sogou', 'sysTran', 'tilde', 'translateCom', 'translateMe', 'utibet', 'volcEngine', 'yandex', 'yeekit', 'youdao']\n"
     ]
    }
   ],
   "source": [
    "print(ts.translators_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e0a08c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello mummy, you are the prettiest! I am so glad I am your son.\n",
      "Bonjour maman, tu es la plus jolie! Je suis tellement content d'Ãªtre ton fils.\n",
      "Hello mom, you are the prettiest! I'm so happy to be your son.\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello mummy, you are the prettiest! I am so glad I am your son.\"\n",
    "_translator = \"google\" #| \"google\"\n",
    "intermediate_language = \"fr\" # \"fr\"\n",
    "\n",
    "txt_translated = ts.translate_text(\n",
    "    txt,\n",
    "    translator=_translator,\n",
    "    from_language=\"en\",\n",
    "    to_language=intermediate_language,\n",
    ")\n",
    "\n",
    "txt_back_translated = ts.translate_text(\n",
    "    txt_translated,\n",
    "    translator=_translator,\n",
    "    from_language=intermediate_language,\n",
    "    to_language=\"en\",\n",
    ")\n",
    "\n",
    "print(txt)\n",
    "print(txt_translated)\n",
    "print(txt_back_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3f2c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(\n",
    "    txt: str,\n",
    "    translator: str,\n",
    "    intermediate_language: str, \n",
    "    original_language: str=\"en\"\n",
    "    ) -> str:\n",
    "\n",
    "    txt_translated = ts.translate_text(\n",
    "        txt,\n",
    "        translator=translator,\n",
    "        from_language=original_language,\n",
    "        to_language=intermediate_language,\n",
    "    )\n",
    "\n",
    "    txt_back_translated = ts.translate_text(\n",
    "        txt_translated,\n",
    "        translator=translator,\n",
    "        from_language=intermediate_language,\n",
    "        to_language=original_language,\n",
    "    )\n",
    "\n",
    "    return txt_back_translated\n",
    "\n",
    "def augment_with_backtranslation(\n",
    "    txt:str,\n",
    "    translators: Sequence[str],\n",
    "    intermediate_languages: Sequence[str],\n",
    "    )-> Sequence[str]:\n",
    "    \"\"\"\n",
    "    Agument the given str by back translating to and from all given intermediate languages\n",
    "    using all the given product (cross product between the two).\n",
    "    Return the created str in lower case.\n",
    "    \"\"\"\n",
    "    new_txts = []\n",
    "    for translator, language in product(translators, intermediate_languages):\n",
    "        new_txt = back_translate(txt, translator, language)\n",
    "        new_txts.append(unescape(new_txt).lower())\n",
    "\n",
    "    new_txts_deduplicated = [new_txt for new_txt in set(new_txts) if new_txt != txt]\n",
    "\n",
    "    return new_txts_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db018f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8c3c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "translators = [\n",
    "    \"google\",\n",
    "    \"alibaba\",\n",
    "]\n",
    "\n",
    "intermediate_languages = [\n",
    "    \"ja\", # Japenese\n",
    "    \"fr\", # French\n",
    "    \"es\", # Spanish\n",
    "    \"zh\", # Chinese\n",
    "    \"de\", # German\n",
    "    \"no\" # Norwegian\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "070ad5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_txts = augment_with_backtranslation(\n",
    "    titles[3],\n",
    "    translators,\n",
    "    intermediate_languages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00b91d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perplexity CEO on Future of AI, Search & the Internet'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b800accd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['confusion -ceo for future of ai, search & the internet',\n",
       " 'ceo of perplexity about the future of ai, the search and internet',\n",
       " 'perplecomplexity ceo on the future of ai, research and the internet',\n",
       " 'the ceo of perplexity on the future of ai, search and the internet',\n",
       " 'ceo of perplexity on the future of ai, search & internet',\n",
       " 'regarding ai, search and the future of the internet confusing ceo',\n",
       " 'ceo at a loss about the future of ai, search & internet',\n",
       " 'ai, search, internet future perplexityceo',\n",
       " 'prison ceo for the future of ai, search and internet',\n",
       " 'ceo in puzzlement over ai, search and internet future',\n",
       " 'future managing director of ai, search & internet',\n",
       " 'ceo confusion on artificial intelligence, search and the future of the internet']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab0552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0713128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_back_translation(\n",
    "    translators=[\"alibaba\", \"google\"],\n",
    "    translators=[\"alibaba\", \"google\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9669e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aef718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexscaper_dev",
   "language": "python",
   "name": "lexscaper_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
